{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2863267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Memory growth enabled for {gpu.name}')\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(91324, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    print(number, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated112/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "\n",
    "    \n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs = 3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42+run)\n",
    "        X_train_bef, X_val, y_train_bef, y_val = train_test_split(X_train_val, y_train_val, test_size=0.42857, random_state=42+run)\n",
    "\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "        \n",
    "        X_train = np.concatenate((X_train_bef, X_new), axis=0)\n",
    "        y_train = np.concatenate((y_train_bef, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    " \n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        with strategy.scope():\n",
    "            model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f9f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Memory growth enabled for {gpu.name}')\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(91324, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    print(number, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated112/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "\n",
    "    \n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs = 3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42+run)\n",
    "    \n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "        \n",
    "        X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "        y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=0.1765, random_state=42+run)\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        with strategy.scope():\n",
    "            model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619c03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "#set_global_policy('mixed_float16')\n",
    "\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "#if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "#    for gpu in gpus:\n",
    "#        try:\n",
    "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#            print(f'Memory growth enabled for {gpu.name}')\n",
    "#        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "#            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(91324, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    for number in range(1000, 11000, 1000): \n",
    "        print(number, 'augmented')  # From 10,000 to 90,000\n",
    "        img_names_new = []  # List to store image names from the second dataset\n",
    "        for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "            num = round(number / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "            for i in range(num):\n",
    "                img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "        X_new = [] \n",
    "        train_new = [] # This will store the new images\n",
    "        y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "        for i in tqdm(range(len(img_names_new))):\n",
    "            img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated112/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "            img = image.load_img(img_path, target_size=(112, 112, 3))\n",
    "            img = image.img_to_array(img)\n",
    "            train_new.append(img)  # Append the processed image\n",
    "\n",
    "            # Assign labels based on the disease class\n",
    "            for j in range(len(disease_class.keys())):\n",
    "                if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                    y_new[i][j] = 1\n",
    "\n",
    "        X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "        ratio=number/91324\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        import numpy as np\n",
    "        import os\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "        #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "        from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "        #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "        #set_global_policy('mixed_float16')\n",
    "        input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "        num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "        #strategy = tf.distribute.MirroredStrategy()\n",
    "        #print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "        # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "      # Your model creation and compilation\n",
    "\n",
    "        # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "        # Function to define and compile the model\n",
    "\n",
    "        def build_model(input_shape, num_classes):\n",
    "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            model = Sequential([\n",
    "                base_model,\n",
    "                GlobalAveragePooling2D(),\n",
    "                Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "            return model\n",
    "        # Function for Focal Loss\n",
    "        #https://www.programmersought.com/article/60001511310/\n",
    "        def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "            epsilon = 1.e-7\n",
    "            def loss_fn2(y_true, y_pred):\n",
    "                y_true = tf.cast(y_true, tf.float32)\n",
    "                y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "                alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "                y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "                ce = -tf.math.log(y_t)\n",
    "                weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "                fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "                loss = tf.reduce_mean(fl)\n",
    "                return loss\n",
    "\n",
    "            return loss_fn2\n",
    "\n",
    "        # Number of runs to calculate the standard deviation\n",
    "        n_runs = 3\n",
    "        runno=42\n",
    "        auc_scores = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            tf.keras.backend.clear_session()\n",
    "            firstdiv=(1+ratio)*(0.15)\n",
    "            seconddiv=(firstdiv)/(1+ratio-firstdiv)\n",
    "            print(firstdiv)\n",
    "            print(seconddiv)\n",
    "            \n",
    "            # Assuming X and y are your complete dataset excluding the test set\n",
    "            X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=firstdiv, random_state=42+run)\n",
    "\n",
    "            # Preprocess the test set\n",
    "            X_test = preprocess_input(X_test)\n",
    "\n",
    "            X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "            y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "            # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "            # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=seconddiv, random_state=42+run)\n",
    "            # Preprocess the training and validation sets\n",
    "            X_train = preprocess_input(X_train)\n",
    "            X_val = preprocess_input(X_val)\n",
    "            # Convert the numpy arrays into tf.data.Dataset\n",
    "            with tf.device(\"CPU\"):\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                batch_size = 10  # You can adjust this according to your specific requirements\n",
    "                train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "                val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "                test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            #with strategy.scope():\n",
    "            model = build_model(input_shape, num_classes)\n",
    "\n",
    "            early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "            model.fit(\n",
    "                train_dataset,  # Use the batched and prefetched dataset\n",
    "                epochs=20,  # Adjust based on your dataset and model's performance\n",
    "                validation_data=val_dataset,  # Use the validation dataset\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1  # Set to 0 to reduce log messages\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "            # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "            y_pred = model.predict(test_dataset)\n",
    "            auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                    # Collect all predictions for computing AUC\n",
    "            #all_y_pred = []\n",
    "           # for batch in test_dataset:\n",
    "            #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "            # Convert to a single numpy array\n",
    "          #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "            # Compute AUC assuming y_test is a single numpy array of labels\n",
    "          #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "            auc_scores.append(auc)\n",
    "            print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "            gc.collect()\n",
    "            # Calculate and print the standard deviation of AUC scores\n",
    "        auc_std_dev = np.std(auc_scores)\n",
    "        aauc=np.mean(auc_scores)\n",
    "        print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "        print(\"aauc=\",aauc)\n",
    "        gc.collect()\n",
    "            # Now, X and y contain the images and labels for this iteration\n",
    "            # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c490835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "augmented = [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000]\n",
    "aauc = [\n",
    "    0.7019529448460166, 0.6981793642807048, 0.6965962017994758,\n",
    "    0.6946183463298995, 0.692164595411011, 0.6905014069274252,\n",
    "    0.6901929777484542, 0.6921003819384491, 0.6874990049854891\n",
    "]\n",
    "sdv = [0.0070, 0.0036, 0.0059, 0.0038, 0.0028, 0.0074, 0.0039, 0.0006, 0.0043]\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of Augmented Images')\n",
    "ax1.set_ylabel('AAUC', color=color)\n",
    "ax1.plot(augmented, aauc, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Standard Deviation', color=color)\n",
    "ax2.plot(augmented, sdv, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('AAUC and Standard Deviation vs. Number of Augmented Images')\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the upper and lower bounds for the AAUC considering the standard deviation\n",
    "aauc_upper = np.array(aauc) + np.array(sdv)\n",
    "aauc_lower = np.array(aauc) - np.array(sdv)\n",
    "\n",
    "# Plotting AAUC with highlighted standard deviation range\n",
    "plt.fill_between(augmented, aauc_lower, aauc_upper, color='gray', alpha=0.2)\n",
    "plt.plot(augmented, aauc, '-o', color='red')\n",
    "plt.title('AAUC with Standard Deviation Highlighted')\n",
    "plt.xlabel('Number of Augmented Images')\n",
    "plt.ylabel('AAUC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cad07d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "#set_global_policy('mixed_float16')\n",
    "\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "#if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "#    for gpu in gpus:\n",
    "#        try:\n",
    "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#            print(f'Memory growth enabled for {gpu.name}')\n",
    "#        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "#            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(90000, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    number2=number//9\n",
    "    print(number2, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number2 / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated112/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "    ratio=number2/number\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    #print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs=3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        firstdiv=(1+ratio)*(0.15)\n",
    "        seconddiv=(firstdiv)/(1+ratio-firstdiv)\n",
    "        print(firstdiv)\n",
    "        print(seconddiv)\n",
    "\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=firstdiv, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "        y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=seconddiv, random_state=42+run)\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        #with strategy.scope():\n",
    "        model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6732a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "#set_global_policy('mixed_float16')\n",
    "\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "#if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "#    for gpu in gpus:\n",
    "#        try:\n",
    "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#            print(f'Memory growth enabled for {gpu.name}')\n",
    "#        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "#            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(91324, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    number2=round(number/30)\n",
    "    print(number2, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number2 / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated112/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "    ratio=number2/number\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    #print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs=3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        firstdiv=(1+ratio)*(0.15)\n",
    "        seconddiv=(firstdiv)/(1+ratio-firstdiv)\n",
    "        print(firstdiv)\n",
    "        print(seconddiv)\n",
    "\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=firstdiv, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "        y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=seconddiv, random_state=42+run)\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        #with strategy.scope():\n",
    "        model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777297b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Memory growth enabled for {gpu.name}')\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(30000, 31000, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized224/' + img_names[i], target_size=(224, 224, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    number2=number\n",
    "    #number2=round(number/10)\n",
    "    print(number2, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number2 / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated224/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(224, 224, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "    ratio=number2/number\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs=3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        firstdiv=(1+ratio)*(0.15)\n",
    "        seconddiv=(firstdiv)/(1+ratio-firstdiv)\n",
    "        print(firstdiv)\n",
    "        print(seconddiv)\n",
    "\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=firstdiv, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "        y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=seconddiv, random_state=42+run)\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        with strategy.scope():\n",
    "            model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de94d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "#set_global_policy('mixed_float16')\n",
    "\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "#if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "#    for gpu in gpus:\n",
    "#        try:\n",
    "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#            print(f'Memory growth enabled for {gpu.name}')\n",
    "#        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "#            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(10000, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    number2=round(number/30)\n",
    "    print(number2, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number2 / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated112/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "    ratio=number2/number\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    #print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs=3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        firstdiv=(1+ratio)*(0.15)\n",
    "        seconddiv=(firstdiv)/(1+ratio-firstdiv)\n",
    "        print(firstdiv)\n",
    "        print(seconddiv)\n",
    "\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=firstdiv, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "        y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=seconddiv, random_state=42+run)\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        #with strategy.scope():\n",
    "        model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, average = 'macro')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5d17f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if there are any GPUs available\n",
    "if gpus:\n",
    "    # Iterate over all available GPUs and set memory growth\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Memory growth enabled for {gpu.name}')\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before initializing GPUs\n",
    "            print(f'Could not set memory growth for {gpu.name}: {e}')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#clear memory in case of OOM\n",
    "K.clear_session()\n",
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}\n",
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}\n",
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])\n",
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)\n",
    "\n",
    "data_ref_2 = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\")  # Update the path to the second dataset\n",
    "\n",
    "# Initialize dictionary for the second dataset\n",
    "disease_img_2 = {disease: [] for disease in disease_class.keys()}\n",
    "\n",
    "# Populate the dictionary with image names from the second dataset\n",
    "for i in tqdm(range(len(data_ref_2))):\n",
    "    if \"|\" not in data_ref_2['Finding Labels'][i]:\n",
    "        disease_img_2[data_ref_2['Finding Labels'][i]].append(data_ref_2['Image Index'][i])\n",
    "\n",
    "# If you need a simplified reference for the second dataset as well\n",
    "simp_data_ref_2 = data_ref_2[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref_2.set_index(\"Image Index\", inplace=True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(60000, 70000, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized224/' + img_names[i], target_size=(224, 224, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    number2=round(number/30)\n",
    "    print(number2, 'augmented')  # From 10,000 to 90,000\n",
    "    img_names_new = []  # List to store image names from the second dataset\n",
    "    for dis in disease_img_2.keys():  # Use the dictionary for the second dataset\n",
    "        num = round(number2 / 91324 * len(disease_img_2[dis]))  # Calculate the number of images per disease\n",
    "        for i in range(num):\n",
    "            img_names_new.append(disease_img_2[dis][i])  # Append image names from the second dataset\n",
    "\n",
    "    X_new = [] \n",
    "    train_new = [] # This will store the new images\n",
    "    y_new = np.zeros(shape=(len(img_names_new), len(disease_class.keys())))  # Initialize the new labels array\n",
    "\n",
    "    for i in tqdm(range(len(img_names_new))):\n",
    "        img_path = '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated224/' + img_names_new[i]  # Update with the actual path to your second dataset images\n",
    "        img = image.load_img(img_path, target_size=(224, 224, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_new.append(img)  # Append the processed image\n",
    "\n",
    "        # Assign labels based on the disease class\n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref_2['Finding Labels'][img_names_new[i]]:  # Use the reference for the second dataset\n",
    "                y_new[i][j] = 1\n",
    "\n",
    "    X_new = np.array(train_new)  # Convert the list of images to a numpy array\n",
    "    ratio=number2/number\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    #from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "    #set_global_policy('mixed_float16')\n",
    "    input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    #print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # Open a strategy scope and create your model, compile it, and train it inside this scope\n",
    "\n",
    "  # Your model creation and compilation\n",
    "\n",
    "    # Build, compile, and train your model within the strategy scope\n",
    "\n",
    "    # Function to define and compile the model\n",
    "\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs=3\n",
    "    runno=42\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        firstdiv=(1+ratio)*(0.15)\n",
    "        seconddiv=(firstdiv)/(1+ratio-firstdiv)\n",
    "        print(firstdiv)\n",
    "        print(seconddiv)\n",
    "\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=firstdiv, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        X_train_new = np.concatenate((X_train_val, X_new), axis=0)\n",
    "        y_train_new = np.concatenate((y_train_val, y_new), axis=0)\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_new, y_train_new, test_size=seconddiv, random_state=42+run)\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "        # Convert the numpy arrays into tf.data.Dataset\n",
    "        with tf.device(\"CPU\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            batch_size = 10  # You can adjust this according to your specific requirements\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        #with strategy.scope():\n",
    "        model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset,  # Use the batched and prefetched dataset\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            validation_data=val_dataset,  # Use the validation dataset\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "                # Collect all predictions for computing AUC\n",
    "        #all_y_pred = []\n",
    "       # for batch in test_dataset:\n",
    "        #    all_y_pred.extend(model.predict(batch[0]))  # batch[0] contains the images\n",
    "\n",
    "        # Convert to a single numpy array\n",
    "      #  all_y_pred = np.vstack(all_y_pred)\n",
    "\n",
    "        # Compute AUC assuming y_test is a single numpy array of labels\n",
    "      #  auc = roc_auc_score(y_test, all_y_pred, multi_class='ovo')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "        gc.collect()\n",
    "        # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "    gc.collect()\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae4f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

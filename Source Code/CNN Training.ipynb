{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 17:17:45.509129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 17:17:45.849851: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-19 17:17:47.003755: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-19 17:17:47.003880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-19 17:17:47.003893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False   #ignore FutureWarning\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.import librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout,MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear memory in case of OOM\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. import data and random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dictionary for disease and its index\n",
    "disease_class = {'Atelectasis': 1,\n",
    "                 'Cardiomegaly': 2,\n",
    "                 'Effusion': 3,\n",
    "                 'Infiltration': 4,\n",
    "                 'Mass': 5,\n",
    "                 'Nodule': 6,\n",
    "                 'Pneumonia': 7,\n",
    "                 'Pneumothorax': 8,\n",
    "                 'Consolidation': 9,\n",
    "                 'Edema': 10,\n",
    "                 'Emphysema': 11,\n",
    "                 'Fibrosis': 12,\n",
    "                 'Pleural_Thickening': 13,\n",
    "                 'Hernia': 14,\n",
    "                 'No Finding': 15}\n",
    "\n",
    "disease_rev = {v: k for k, v in disease_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_img = {'Atelectasis': [],\n",
    "                 'Cardiomegaly': [],\n",
    "                 'Effusion': [],\n",
    "                 'Infiltration': [],\n",
    "                 'Mass': [],\n",
    "                 'Nodule': [],\n",
    "                 'Pneumonia': [],\n",
    "                 'Pneumothorax': [],\n",
    "                 'Consolidation': [],\n",
    "                 'Edema': [],\n",
    "                 'Emphysema': [],\n",
    "                 'Fibrosis': [],\n",
    "                 'Pleural_Thickening': [],\n",
    "                 'Hernia': [],\n",
    "                 'No Finding':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 112120/112120 [00:02<00:00, 53705.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#import labels of the images\n",
    "data_ref = pd.read_csv(\"/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\")\n",
    "pd.options.mode.chained_assignment = None        #ignore the SettingWithCopyWarning\n",
    "\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/Generated/reconstructed_labels.csv\n",
    "#/media/ntu/volume1/home/s123md305_01/Documents/CXR8/Data_Entry_2017_v2020.csv\n",
    "for i in tqdm(range(len(data_ref))):\n",
    "    #print(i)\n",
    "    if \"|\" not in data_ref['Finding Labels'][i]:\n",
    "        disease_img[data_ref['Finding Labels'][i]].append(data_ref['Image Index'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_data_ref = data_ref[[\"Image Index\", \"Finding Labels\"]]\n",
    "simp_data_ref.set_index(\"Image Index\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - this section explores multiple different architectures and augmentation ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 10000\n",
    "\n",
    "img_names = []\n",
    "for dis in disease_img.keys():\n",
    "    num = round(number/91324*len(disease_img[dis]))\n",
    "    for i in range(num):\n",
    "        img_names.append(disease_img[dis][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated224/00000011_006.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(img_names), \u001b[38;5;28mlen\u001b[39m(disease_class\u001b[38;5;241m.\u001b[39mkeys())))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(img_names))):\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#img = image.load_img('Noise/N'+img_names[i],target_size=(112,112,3))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated224/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mimg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m112\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m112\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m#GG:cycleGAN; G:DCGAN\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[1;32m      9\u001b[0m     train_image\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/stylegan/lib/python3.8/site-packages/keras/utils/image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated224/00000011_006.png'"
     ]
    }
   ],
   "source": [
    "train_image = []\n",
    "y = np.zeros(shape = (len(img_names), len(disease_class.keys())))\n",
    "\n",
    "for i in tqdm(range(len(img_names))):\n",
    "\n",
    "    #img = image.load_img('Noise/N'+img_names[i],target_size=(112,112,3))\n",
    "    img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/Generated/ComGenerated224/'+img_names[i],target_size=(112,112,3))      #GG:cycleGAN; G:DCGAN\n",
    "    img = image.img_to_array(img)\n",
    "    train_image.append(img)\n",
    "    \n",
    "    for j in range(len(disease_class.keys())):\n",
    "        if disease_rev[j+1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "            y[i][j] = 1\n",
    "            \n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 42, test_size = 0.15, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_input(X_train)\n",
    "X_val = preprocess_input(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function\n",
    "#https://www.programmersought.com/article/60001511310/\n",
    "def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "\n",
    "    epsilon = 1.e-7\n",
    "    def loss_fn2(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    \n",
    "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.math.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "    \n",
    "    return loss_fn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 10000\n",
    "\n",
    "img_names = []\n",
    "for dis in disease_img.keys():\n",
    "    num = round(number/91324*len(disease_img[dis]))\n",
    "    for i in range(num):\n",
    "        img_names.append(disease_img[dis][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [02:55<00:00, 57.08it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "train_image = []\n",
    "y = np.zeros(shape = (len(img_names), len(disease_class.keys())))\n",
    "\n",
    "for i in tqdm(range(len(img_names))):\n",
    "    \n",
    "    img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/Dataset/'+img_names[i],target_size=(224,224,3))\n",
    "    img = image.img_to_array(img)\n",
    "    train_image.append(img)\n",
    "    \n",
    "    for j in range(len(disease_class.keys())):\n",
    "        if disease_rev[j+1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "            y[i][j] = 1\n",
    "            \n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_input(X_train)\n",
    "X_test = preprocess_input(X_test)\n",
    "y_train = preprocess_input(y_train)\n",
    "y_test = preprocess_input(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "700/700 [==============================] - 22s 24ms/step - loss: 3.6492 - auc: 0.8736 - val_loss: 3.0480 - val_auc: 0.9023\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 3.0700 - auc: 0.9020 - val_loss: 2.9399 - val_auc: 0.9052\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.8640 - auc: 0.9130 - val_loss: 2.8560 - val_auc: 0.9108\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.7051 - auc: 0.9222 - val_loss: 2.8426 - val_auc: 0.9123\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.5957 - auc: 0.9284 - val_loss: 2.7998 - val_auc: 0.9146\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.4974 - auc: 0.9338 - val_loss: 2.8559 - val_auc: 0.9105\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.4270 - auc: 0.9374 - val_loss: 2.7762 - val_auc: 0.9165\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.3400 - auc: 0.9420 - val_loss: 2.7648 - val_auc: 0.9182\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.2889 - auc: 0.9448 - val_loss: 2.7892 - val_auc: 0.9147\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.2264 - auc: 0.9478 - val_loss: 2.7505 - val_auc: 0.9174\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.1641 - auc: 0.9507 - val_loss: 2.8014 - val_auc: 0.9178\n",
      "Epoch 11: early stopping\n",
      "47/47 [==============================] - 3s 19ms/step\n",
      "Run 1/5, Test AUC: 0.6696\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 20s 21ms/step - loss: 3.8472 - auc: 0.8636 - val_loss: 3.1986 - val_auc: 0.8962\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 3.0597 - auc: 0.9022 - val_loss: 3.0144 - val_auc: 0.9068\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.8605 - auc: 0.9132 - val_loss: 2.9752 - val_auc: 0.9100\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.7193 - auc: 0.9206 - val_loss: 2.8820 - val_auc: 0.9104\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.6110 - auc: 0.9269 - val_loss: 2.9201 - val_auc: 0.9080\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.5224 - auc: 0.9316 - val_loss: 2.8716 - val_auc: 0.9114\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.4391 - auc: 0.9360 - val_loss: 2.9762 - val_auc: 0.9115\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.3637 - auc: 0.9402 - val_loss: 2.8630 - val_auc: 0.9127\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.3007 - auc: 0.9436 - val_loss: 2.9223 - val_auc: 0.9081\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.2507 - auc: 0.9460 - val_loss: 2.8927 - val_auc: 0.9103\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.1785 - auc: 0.9495 - val_loss: 2.9046 - val_auc: 0.9109\n",
      "Epoch 11: early stopping\n",
      "47/47 [==============================] - 2s 14ms/step\n",
      "Run 2/5, Test AUC: 0.6598\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 20s 21ms/step - loss: 3.9085 - auc: 0.8622 - val_loss: 3.0873 - val_auc: 0.8989\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 3.1425 - auc: 0.8985 - val_loss: 2.9453 - val_auc: 0.9064\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.9060 - auc: 0.9109 - val_loss: 2.8390 - val_auc: 0.9118\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.7641 - auc: 0.9184 - val_loss: 2.7794 - val_auc: 0.9138\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.6464 - auc: 0.9254 - val_loss: 2.8081 - val_auc: 0.9114\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.5491 - auc: 0.9310 - val_loss: 2.8585 - val_auc: 0.9165\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.4601 - auc: 0.9356 - val_loss: 2.7962 - val_auc: 0.9141\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.3831 - auc: 0.9395 - val_loss: 2.8470 - val_auc: 0.9189\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.3163 - auc: 0.9432 - val_loss: 2.7671 - val_auc: 0.9158\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.2516 - auc: 0.9460 - val_loss: 2.7840 - val_auc: 0.9147\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.2048 - auc: 0.9488 - val_loss: 2.7822 - val_auc: 0.9166\n",
      "Epoch 11: early stopping\n",
      "47/47 [==============================] - 2s 14ms/step\n",
      "Run 3/5, Test AUC: 0.6712\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 22s 22ms/step - loss: 3.6545 - auc: 0.8750 - val_loss: 3.3689 - val_auc: 0.8898\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 13s 18ms/step - loss: 3.0014 - auc: 0.9062 - val_loss: 3.2371 - val_auc: 0.8933\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 13s 18ms/step - loss: 2.7930 - auc: 0.9170 - val_loss: 3.1978 - val_auc: 0.8990\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 13s 18ms/step - loss: 2.6668 - auc: 0.9239 - val_loss: 3.1570 - val_auc: 0.9000\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 13s 18ms/step - loss: 2.5515 - auc: 0.9302 - val_loss: 3.0947 - val_auc: 0.9048\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.4553 - auc: 0.9354 - val_loss: 3.1244 - val_auc: 0.8973\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.3844 - auc: 0.9388 - val_loss: 3.0703 - val_auc: 0.9049\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.3182 - auc: 0.9425 - val_loss: 3.0191 - val_auc: 0.9043\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.2493 - auc: 0.9458 - val_loss: 3.0493 - val_auc: 0.9031\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 13s 18ms/step - loss: 2.1933 - auc: 0.9490 - val_loss: 3.0381 - val_auc: 0.9021\n",
      "Epoch 10: early stopping\n",
      "47/47 [==============================] - 2s 15ms/step\n",
      "Run 4/5, Test AUC: 0.6443\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 20s 21ms/step - loss: 3.8376 - auc: 0.8631 - val_loss: 3.1565 - val_auc: 0.8986\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 3.0527 - auc: 0.9027 - val_loss: 2.9611 - val_auc: 0.9087\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.8405 - auc: 0.9145 - val_loss: 2.9079 - val_auc: 0.9126\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.7043 - auc: 0.9218 - val_loss: 2.8372 - val_auc: 0.9145\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.6029 - auc: 0.9274 - val_loss: 2.8521 - val_auc: 0.9167\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.5052 - auc: 0.9329 - val_loss: 2.7969 - val_auc: 0.9172\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.4261 - auc: 0.9370 - val_loss: 2.7768 - val_auc: 0.9166\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 12s 18ms/step - loss: 2.3552 - auc: 0.9411 - val_loss: 2.8263 - val_auc: 0.9142\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 2.2799 - auc: 0.9448 - val_loss: 2.7638 - val_auc: 0.9164\n",
      "Epoch 9: early stopping\n",
      "47/47 [==============================] - 2s 14ms/step\n",
      "Run 5/5, Test AUC: 0.6653\n",
      "Standard Deviation of AUC over 5 runs: 0.0097\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "\n",
    "# Function to define and compile the model\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Function for Focal Loss\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_sum(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) - tf.reduce_sum((1-alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Number of runs to calculate the standard deviation\n",
    "n_runs = 5\n",
    "auc_scores = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Assuming X and y are your complete dataset excluding the test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "    # Preprocess the test set\n",
    "    X_test = preprocess_input(X_test)\n",
    "   \n",
    "    # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "    # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "    # Preprocess the training and validation sets\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_val = preprocess_input(X_val)\n",
    "\n",
    "    model = build_model(input_shape, num_classes)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,  # Adjust based on your dataset and model's performance\n",
    "        batch_size=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1  # Set to 0 to reduce log messages\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate and print the standard deviation of AUC scores\n",
    "auc_std_dev = np.std(auc_scores)\n",
    "print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "700/700 [==============================] - 29s 38ms/step - loss: 0.0931 - auc: 0.7912 - val_loss: 0.0680 - val_auc: 0.8503\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0684 - auc: 0.8411 - val_loss: 0.0593 - val_auc: 0.8720\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0598 - auc: 0.8565 - val_loss: 0.0540 - val_auc: 0.8750\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0543 - auc: 0.8672 - val_loss: 0.0500 - val_auc: 0.8806\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0503 - auc: 0.8764 - val_loss: 0.0473 - val_auc: 0.8873\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0473 - auc: 0.8842 - val_loss: 0.0462 - val_auc: 0.8910\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0448 - auc: 0.8908 - val_loss: 0.0440 - val_auc: 0.8930\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0428 - auc: 0.8962 - val_loss: 0.0426 - val_auc: 0.8983\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0410 - auc: 0.9015 - val_loss: 0.0419 - val_auc: 0.8963\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0396 - auc: 0.9050 - val_loss: 0.0408 - val_auc: 0.9008\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0382 - auc: 0.9094 - val_loss: 0.0401 - val_auc: 0.9014\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0370 - auc: 0.9130 - val_loss: 0.0402 - val_auc: 0.8992\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0361 - auc: 0.9154 - val_loss: 0.0398 - val_auc: 0.9037\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0351 - auc: 0.9189 - val_loss: 0.0387 - val_auc: 0.9049\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0343 - auc: 0.9214 - val_loss: 0.0382 - val_auc: 0.9096\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0335 - auc: 0.9242 - val_loss: 0.0377 - val_auc: 0.9073\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0329 - auc: 0.9260 - val_loss: 0.0376 - val_auc: 0.9122\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0322 - auc: 0.9285 - val_loss: 0.0378 - val_auc: 0.9077\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0315 - auc: 0.9305 - val_loss: 0.0366 - val_auc: 0.9122\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0309 - auc: 0.9326 - val_loss: 0.0370 - val_auc: 0.9084\n",
      "Epoch 20: early stopping\n",
      "47/47 [==============================] - 8s 115ms/step\n",
      "Run 1/5, Test AUC: 0.6450\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 27s 36ms/step - loss: 0.0976 - auc: 0.7893 - val_loss: 0.0742 - val_auc: 0.8309\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0652 - auc: 0.8552 - val_loss: 0.0621 - val_auc: 0.8584\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0572 - auc: 0.8668 - val_loss: 0.0562 - val_auc: 0.8655\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0524 - auc: 0.8759 - val_loss: 0.0534 - val_auc: 0.8686\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0489 - auc: 0.8825 - val_loss: 0.0503 - val_auc: 0.8776\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0464 - auc: 0.8882 - val_loss: 0.0485 - val_auc: 0.8852\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0441 - auc: 0.8939 - val_loss: 0.0467 - val_auc: 0.8891\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0422 - auc: 0.8990 - val_loss: 0.0453 - val_auc: 0.8884\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0405 - auc: 0.9033 - val_loss: 0.0443 - val_auc: 0.8948\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0393 - auc: 0.9069 - val_loss: 0.0435 - val_auc: 0.8958\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0380 - auc: 0.9102 - val_loss: 0.0421 - val_auc: 0.8938\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0368 - auc: 0.9135 - val_loss: 0.0416 - val_auc: 0.8952\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0358 - auc: 0.9168 - val_loss: 0.0410 - val_auc: 0.8997\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0349 - auc: 0.9195 - val_loss: 0.0400 - val_auc: 0.8995\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0340 - auc: 0.9225 - val_loss: 0.0402 - val_auc: 0.8987\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0334 - auc: 0.9239 - val_loss: 0.0391 - val_auc: 0.9047\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0326 - auc: 0.9264 - val_loss: 0.0395 - val_auc: 0.9003\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0320 - auc: 0.9286 - val_loss: 0.0387 - val_auc: 0.9026\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0315 - auc: 0.9298 - val_loss: 0.0387 - val_auc: 0.9068\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0309 - auc: 0.9317 - val_loss: 0.0380 - val_auc: 0.9053\n",
      "47/47 [==============================] - 3s 60ms/step\n",
      "Run 2/5, Test AUC: 0.6605\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 27s 36ms/step - loss: 0.0763 - auc: 0.8294 - val_loss: 0.0651 - val_auc: 0.8431\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0619 - auc: 0.8553 - val_loss: 0.0580 - val_auc: 0.8592\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0557 - auc: 0.8666 - val_loss: 0.0545 - val_auc: 0.8692\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0518 - auc: 0.8739 - val_loss: 0.0512 - val_auc: 0.8749\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0486 - auc: 0.8824 - val_loss: 0.0500 - val_auc: 0.8729\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0461 - auc: 0.8883 - val_loss: 0.0471 - val_auc: 0.8837\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0440 - auc: 0.8942 - val_loss: 0.0457 - val_auc: 0.8895\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0422 - auc: 0.8988 - val_loss: 0.0446 - val_auc: 0.8876\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0408 - auc: 0.9024 - val_loss: 0.0433 - val_auc: 0.8956\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0394 - auc: 0.9065 - val_loss: 0.0428 - val_auc: 0.8936\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0383 - auc: 0.9097 - val_loss: 0.0418 - val_auc: 0.8992\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0372 - auc: 0.9129 - val_loss: 0.0426 - val_auc: 0.8917\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0362 - auc: 0.9159 - val_loss: 0.0405 - val_auc: 0.9008\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0354 - auc: 0.9187 - val_loss: 0.0401 - val_auc: 0.9035\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0345 - auc: 0.9213 - val_loss: 0.0396 - val_auc: 0.9041\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0338 - auc: 0.9238 - val_loss: 0.0399 - val_auc: 0.8999\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0334 - auc: 0.9251 - val_loss: 0.0386 - val_auc: 0.9079\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0325 - auc: 0.9278 - val_loss: 0.0385 - val_auc: 0.9098\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0320 - auc: 0.9295 - val_loss: 0.0392 - val_auc: 0.9036\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0315 - auc: 0.9313 - val_loss: 0.0380 - val_auc: 0.9068\n",
      " 4/47 [=>............................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 20:51:50.769134: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:50.808469: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:50.882416: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:50.948723: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/47 [====>.........................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 20:51:51.011620: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:51.076400: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:51.144860: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:51.207858: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/47 [======>.......................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 20:51:51.271289: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-10 20:51:51.335200: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 884.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 3s 67ms/step\n",
      "Run 3/5, Test AUC: 0.6663\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 26s 35ms/step - loss: 0.0947 - auc: 0.7941 - val_loss: 0.0721 - val_auc: 0.8381\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0658 - auc: 0.8506 - val_loss: 0.0639 - val_auc: 0.8539\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0582 - auc: 0.8646 - val_loss: 0.0594 - val_auc: 0.8549\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0535 - auc: 0.8740 - val_loss: 0.0562 - val_auc: 0.8711\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0498 - auc: 0.8820 - val_loss: 0.0534 - val_auc: 0.8713\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0471 - auc: 0.8879 - val_loss: 0.0507 - val_auc: 0.8755\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0447 - auc: 0.8936 - val_loss: 0.0493 - val_auc: 0.8747\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0426 - auc: 0.8991 - val_loss: 0.0479 - val_auc: 0.8834\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0409 - auc: 0.9036 - val_loss: 0.0464 - val_auc: 0.8843\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0393 - auc: 0.9078 - val_loss: 0.0464 - val_auc: 0.8892\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0380 - auc: 0.9115 - val_loss: 0.0446 - val_auc: 0.8902\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0368 - auc: 0.9146 - val_loss: 0.0438 - val_auc: 0.8924\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0357 - auc: 0.9180 - val_loss: 0.0434 - val_auc: 0.8919\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0347 - auc: 0.9210 - val_loss: 0.0431 - val_auc: 0.8919\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0339 - auc: 0.9234 - val_loss: 0.0422 - val_auc: 0.8980\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0331 - auc: 0.9262 - val_loss: 0.0419 - val_auc: 0.8956\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0323 - auc: 0.9282 - val_loss: 0.0416 - val_auc: 0.8948\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0317 - auc: 0.9303 - val_loss: 0.0412 - val_auc: 0.8989\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0310 - auc: 0.9323 - val_loss: 0.0403 - val_auc: 0.8988\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0305 - auc: 0.9341 - val_loss: 0.0399 - val_auc: 0.9024\n",
      "47/47 [==============================] - 3s 61ms/step\n",
      "Run 4/5, Test AUC: 0.6587\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 27s 36ms/step - loss: 0.0911 - auc: 0.8171 - val_loss: 0.0701 - val_auc: 0.8416\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0669 - auc: 0.8485 - val_loss: 0.0615 - val_auc: 0.8630\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0587 - auc: 0.8635 - val_loss: 0.0556 - val_auc: 0.8747\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0536 - auc: 0.8736 - val_loss: 0.0521 - val_auc: 0.8761\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0499 - auc: 0.8809 - val_loss: 0.0490 - val_auc: 0.8871\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0471 - auc: 0.8880 - val_loss: 0.0470 - val_auc: 0.8911\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0447 - auc: 0.8924 - val_loss: 0.0459 - val_auc: 0.8963\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0429 - auc: 0.8975 - val_loss: 0.0441 - val_auc: 0.8953\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0411 - auc: 0.9023 - val_loss: 0.0425 - val_auc: 0.8994\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0397 - auc: 0.9059 - val_loss: 0.0434 - val_auc: 0.8935\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0384 - auc: 0.9095 - val_loss: 0.0420 - val_auc: 0.8975\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0372 - auc: 0.9130 - val_loss: 0.0404 - val_auc: 0.9038\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0362 - auc: 0.9156 - val_loss: 0.0402 - val_auc: 0.9039\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0351 - auc: 0.9188 - val_loss: 0.0409 - val_auc: 0.9014\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0343 - auc: 0.9216 - val_loss: 0.0392 - val_auc: 0.9044\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0335 - auc: 0.9235 - val_loss: 0.0378 - val_auc: 0.9121\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0328 - auc: 0.9259 - val_loss: 0.0375 - val_auc: 0.9127\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0321 - auc: 0.9285 - val_loss: 0.0388 - val_auc: 0.9055\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0315 - auc: 0.9302 - val_loss: 0.0374 - val_auc: 0.9101\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 23s 32ms/step - loss: 0.0309 - auc: 0.9322 - val_loss: 0.0365 - val_auc: 0.9165\n",
      "47/47 [==============================] - 3s 61ms/step\n",
      "Run 5/5, Test AUC: 0.6518\n",
      "Standard Deviation of AUC over 5 runs: 0.0074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "\n",
    "# Function to define and compile the model\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Function for Focal Loss\n",
    "#https://www.programmersought.com/article/60001511310/\n",
    "def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "    epsilon = 1.e-7\n",
    "    def loss_fn2(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    \n",
    "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.math.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "    \n",
    "    return loss_fn2\n",
    "\n",
    "# Number of runs to calculate the standard deviation\n",
    "n_runs = 5\n",
    "auc_scores = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Assuming X and y are your complete dataset excluding the test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "    # Preprocess the test set\n",
    "    X_test = preprocess_input(X_test)\n",
    "   \n",
    "    # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "    # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "    # Preprocess the training and validation sets\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_val = preprocess_input(X_val)\n",
    "\n",
    "    model = build_model(input_shape, num_classes)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,  # Adjust based on your dataset and model's performance\n",
    "        batch_size=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1  # Set to 0 to reduce log messages\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate and print the standard deviation of AUC scores\n",
    "auc_std_dev = np.std(auc_scores)\n",
    "print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 19:31:10.673250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 19:31:15.262703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:19:00.0, compute capability: 8.6\n",
      "2024-03-10 19:31:15.264508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22288 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2024-03-10 19:31:15.266261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22288 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:67:00.0, compute capability: 8.6\n",
      "2024-03-10 19:31:15.267989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22228 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:68:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 19:31:40.602781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n",
      "2024-03-10 19:31:42.364200: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-10 19:31:44.148029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-10 19:31:44.152892: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x557046f4db00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-10 19:31:44.152930: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-10 19:31:44.152937: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-10 19:31:44.152942: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-10 19:31:44.152946: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-10 19:31:44.159953: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-10 19:31:44.298200: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-10 19:31:44.377706: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 31s 29ms/step - loss: 0.0404 - auc: 0.8856 - val_loss: 0.0340 - val_auc: 0.9123\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0340 - auc: 0.9142 - val_loss: 0.0331 - val_auc: 0.9177\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0323 - auc: 0.9224 - val_loss: 0.0323 - val_auc: 0.9204\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0313 - auc: 0.9270 - val_loss: 0.0322 - val_auc: 0.9214\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0303 - auc: 0.9320 - val_loss: 0.0322 - val_auc: 0.9254\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0294 - auc: 0.9356 - val_loss: 0.0323 - val_auc: 0.9220\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0288 - auc: 0.9381 - val_loss: 0.0311 - val_auc: 0.9267\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0281 - auc: 0.9410 - val_loss: 0.0313 - val_auc: 0.9255\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0276 - auc: 0.9432 - val_loss: 0.0315 - val_auc: 0.9240\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0269 - auc: 0.9457 - val_loss: 0.0312 - val_auc: 0.9254\n",
      "Epoch 10: early stopping\n",
      "47/47 [==============================] - 4s 46ms/step\n",
      "Run 1/5, Test AUC: 0.7122\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 26s 29ms/step - loss: 0.0404 - auc: 0.8855 - val_loss: 0.0348 - val_auc: 0.9130\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0340 - auc: 0.9149 - val_loss: 0.0337 - val_auc: 0.9164\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0323 - auc: 0.9223 - val_loss: 0.0335 - val_auc: 0.9181\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0310 - auc: 0.9283 - val_loss: 0.0325 - val_auc: 0.9220\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0301 - auc: 0.9327 - val_loss: 0.0326 - val_auc: 0.9218\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0292 - auc: 0.9365 - val_loss: 0.0324 - val_auc: 0.9222\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0285 - auc: 0.9395 - val_loss: 0.0320 - val_auc: 0.9246\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0279 - auc: 0.9422 - val_loss: 0.0323 - val_auc: 0.9242\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0272 - auc: 0.9447 - val_loss: 0.0326 - val_auc: 0.9213\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0267 - auc: 0.9468 - val_loss: 0.0318 - val_auc: 0.9240\n",
      "Epoch 10: early stopping\n",
      "47/47 [==============================] - 3s 40ms/step\n",
      "Run 2/5, Test AUC: 0.7073\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 25s 29ms/step - loss: 0.0382 - auc: 0.8964 - val_loss: 0.0345 - val_auc: 0.9115\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0343 - auc: 0.9134 - val_loss: 0.0335 - val_auc: 0.9167\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0328 - auc: 0.9207 - val_loss: 0.0322 - val_auc: 0.9208\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0316 - auc: 0.9264 - val_loss: 0.0327 - val_auc: 0.9183\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0307 - auc: 0.9306 - val_loss: 0.0321 - val_auc: 0.9231\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0299 - auc: 0.9346 - val_loss: 0.0315 - val_auc: 0.9243\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0291 - auc: 0.9374 - val_loss: 0.0313 - val_auc: 0.9258\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0285 - auc: 0.9404 - val_loss: 0.0316 - val_auc: 0.9242\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0279 - auc: 0.9427 - val_loss: 0.0312 - val_auc: 0.9253\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0273 - auc: 0.9453 - val_loss: 0.0320 - val_auc: 0.9243\n",
      "Epoch 10: early stopping\n",
      "47/47 [==============================] - 3s 40ms/step\n",
      "Run 3/5, Test AUC: 0.7082\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 26s 29ms/step - loss: 0.0414 - auc: 0.8836 - val_loss: 0.0376 - val_auc: 0.9011\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0340 - auc: 0.9149 - val_loss: 0.0363 - val_auc: 0.9060\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0324 - auc: 0.9218 - val_loss: 0.0348 - val_auc: 0.9117\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0312 - auc: 0.9275 - val_loss: 0.0350 - val_auc: 0.9130\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0302 - auc: 0.9322 - val_loss: 0.0348 - val_auc: 0.9120\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0293 - auc: 0.9363 - val_loss: 0.0342 - val_auc: 0.9182\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0286 - auc: 0.9391 - val_loss: 0.0338 - val_auc: 0.9155\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0280 - auc: 0.9416 - val_loss: 0.0333 - val_auc: 0.9189\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0274 - auc: 0.9442 - val_loss: 0.0336 - val_auc: 0.9198\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0268 - auc: 0.9460 - val_loss: 0.0333 - val_auc: 0.9194\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0263 - auc: 0.9484 - val_loss: 0.0333 - val_auc: 0.9209\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0259 - auc: 0.9499 - val_loss: 0.0339 - val_auc: 0.9156\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 15s 22ms/step - loss: 0.0254 - auc: 0.9514 - val_loss: 0.0339 - val_auc: 0.9208\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0250 - auc: 0.9535 - val_loss: 0.0333 - val_auc: 0.9190\n",
      "Epoch 14: early stopping\n",
      "47/47 [==============================] - 3s 40ms/step\n",
      "Run 4/5, Test AUC: 0.7125\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 25s 29ms/step - loss: 0.0415 - auc: 0.8852 - val_loss: 0.0352 - val_auc: 0.9093\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0345 - auc: 0.9129 - val_loss: 0.0335 - val_auc: 0.9159\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0326 - auc: 0.9211 - val_loss: 0.0321 - val_auc: 0.9235\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0314 - auc: 0.9273 - val_loss: 0.0320 - val_auc: 0.9232\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0304 - auc: 0.9315 - val_loss: 0.0318 - val_auc: 0.9256\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0295 - auc: 0.9361 - val_loss: 0.0313 - val_auc: 0.9273\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0289 - auc: 0.9386 - val_loss: 0.0308 - val_auc: 0.9294\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0282 - auc: 0.9410 - val_loss: 0.0305 - val_auc: 0.9300\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 16s 23ms/step - loss: 0.0275 - auc: 0.9440 - val_loss: 0.0306 - val_auc: 0.9305\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 16s 23ms/step - loss: 0.0270 - auc: 0.9460 - val_loss: 0.0308 - val_auc: 0.9287\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0264 - auc: 0.9481 - val_loss: 0.0305 - val_auc: 0.9307\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0260 - auc: 0.9495 - val_loss: 0.0304 - val_auc: 0.9306\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0256 - auc: 0.9511 - val_loss: 0.0302 - val_auc: 0.9312\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0252 - auc: 0.9528 - val_loss: 0.0306 - val_auc: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0248 - auc: 0.9543 - val_loss: 0.0301 - val_auc: 0.9315\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0244 - auc: 0.9554 - val_loss: 0.0303 - val_auc: 0.9313\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0239 - auc: 0.9571 - val_loss: 0.0300 - val_auc: 0.9326\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0236 - auc: 0.9580 - val_loss: 0.0310 - val_auc: 0.9281\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 16s 23ms/step - loss: 0.0233 - auc: 0.9594 - val_loss: 0.0302 - val_auc: 0.9316\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 16s 22ms/step - loss: 0.0230 - auc: 0.9602 - val_loss: 0.0304 - val_auc: 0.9304\n",
      "Epoch 20: early stopping\n",
      "47/47 [==============================] - 4s 40ms/step\n",
      "Run 5/5, Test AUC: 0.7109\n",
      "Standard Deviation of AUC over 5 runs: 0.0021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "\n",
    "# Function to define and compile the model\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Function for Focal Loss\n",
    "#https://www.programmersought.com/article/60001511310/\n",
    "def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "    epsilon = 1.e-7\n",
    "    def loss_fn2(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    \n",
    "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.math.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "    \n",
    "    return loss_fn2\n",
    "\n",
    "# Number of runs to calculate the standard deviation\n",
    "n_runs = 5\n",
    "auc_scores = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Assuming X and y are your complete dataset excluding the test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "    # Preprocess the test set\n",
    "    X_test = preprocess_input(X_test)\n",
    "   \n",
    "    # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "    # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "    # Preprocess the training and validation sets\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_val = preprocess_input(X_val)\n",
    "\n",
    "    model = build_model(input_shape, num_classes)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,  # Adjust based on your dataset and model's performance\n",
    "        batch_size=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1  # Set to 0 to reduce log messages\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate and print the standard deviation of AUC scores\n",
    "auc_std_dev = np.std(auc_scores)\n",
    "print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "700/700 [==============================] - 38s 38ms/step - loss: 0.0428 - auc: 0.8726 - val_loss: 0.0356 - val_auc: 0.9074\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0365 - auc: 0.9012 - val_loss: 0.0342 - val_auc: 0.9123\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0352 - auc: 0.9072 - val_loss: 0.0335 - val_auc: 0.9155\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0343 - auc: 0.9120 - val_loss: 0.0327 - val_auc: 0.9188\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0335 - auc: 0.9157 - val_loss: 0.0322 - val_auc: 0.9212\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0329 - auc: 0.9186 - val_loss: 0.0320 - val_auc: 0.9215\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0323 - auc: 0.9217 - val_loss: 0.0322 - val_auc: 0.9211\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0319 - auc: 0.9237 - val_loss: 0.0316 - val_auc: 0.9239\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0315 - auc: 0.9258 - val_loss: 0.0314 - val_auc: 0.9246\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0311 - auc: 0.9275 - val_loss: 0.0312 - val_auc: 0.9262\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0308 - auc: 0.9290 - val_loss: 0.0309 - val_auc: 0.9273\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0304 - auc: 0.9308 - val_loss: 0.0312 - val_auc: 0.9258\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0301 - auc: 0.9320 - val_loss: 0.0309 - val_auc: 0.9270\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0299 - auc: 0.9333 - val_loss: 0.0312 - val_auc: 0.9246\n",
      "Epoch 14: early stopping\n",
      "47/47 [==============================] - 7s 63ms/step\n",
      "Run 1/5, Test AUC: 0.6943\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 36s 37ms/step - loss: 0.0414 - auc: 0.8787 - val_loss: 0.0369 - val_auc: 0.9006\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0355 - auc: 0.9055 - val_loss: 0.0355 - val_auc: 0.9063\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0344 - auc: 0.9108 - val_loss: 0.0348 - val_auc: 0.9102\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0335 - auc: 0.9150 - val_loss: 0.0343 - val_auc: 0.9117\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0329 - auc: 0.9182 - val_loss: 0.0338 - val_auc: 0.9142\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0323 - auc: 0.9211 - val_loss: 0.0338 - val_auc: 0.9144\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0318 - auc: 0.9236 - val_loss: 0.0332 - val_auc: 0.9161\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0314 - auc: 0.9256 - val_loss: 0.0331 - val_auc: 0.9173\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0310 - auc: 0.9274 - val_loss: 0.0329 - val_auc: 0.9179\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0307 - auc: 0.9293 - val_loss: 0.0330 - val_auc: 0.9177\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0304 - auc: 0.9307 - val_loss: 0.0327 - val_auc: 0.9199\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0301 - auc: 0.9319 - val_loss: 0.0330 - val_auc: 0.9188\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0298 - auc: 0.9336 - val_loss: 0.0328 - val_auc: 0.9193\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0296 - auc: 0.9343 - val_loss: 0.0325 - val_auc: 0.9204\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0293 - auc: 0.9355 - val_loss: 0.0322 - val_auc: 0.9218\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0291 - auc: 0.9367 - val_loss: 0.0324 - val_auc: 0.9212\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0288 - auc: 0.9378 - val_loss: 0.0322 - val_auc: 0.9219\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0286 - auc: 0.9385 - val_loss: 0.0324 - val_auc: 0.9211\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 21s 31ms/step - loss: 0.0284 - auc: 0.9393 - val_loss: 0.0321 - val_auc: 0.9223\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 21s 30ms/step - loss: 0.0283 - auc: 0.9401 - val_loss: 0.0321 - val_auc: 0.9226\n",
      "47/47 [==============================] - 5s 47ms/step\n",
      "Run 2/5, Test AUC: 0.7066\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 36s 39ms/step - loss: 0.0406 - auc: 0.8835 - val_loss: 0.0362 - val_auc: 0.9007\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0365 - auc: 0.9013 - val_loss: 0.0351 - val_auc: 0.9056\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0353 - auc: 0.9073 - val_loss: 0.0343 - val_auc: 0.9099\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0344 - auc: 0.9118 - val_loss: 0.0337 - val_auc: 0.9137\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0337 - auc: 0.9155 - val_loss: 0.0332 - val_auc: 0.9149\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0331 - auc: 0.9182 - val_loss: 0.0332 - val_auc: 0.9173\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0326 - auc: 0.9205 - val_loss: 0.0325 - val_auc: 0.9186\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0322 - auc: 0.9228 - val_loss: 0.0324 - val_auc: 0.9190\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0318 - auc: 0.9244 - val_loss: 0.0321 - val_auc: 0.9213\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0315 - auc: 0.9266 - val_loss: 0.0318 - val_auc: 0.9220\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0312 - auc: 0.9276 - val_loss: 0.0318 - val_auc: 0.9224\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0309 - auc: 0.9296 - val_loss: 0.0318 - val_auc: 0.9234\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0306 - auc: 0.9305 - val_loss: 0.0317 - val_auc: 0.9244\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0303 - auc: 0.9320 - val_loss: 0.0317 - val_auc: 0.9223\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0300 - auc: 0.9331 - val_loss: 0.0316 - val_auc: 0.9240\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0298 - auc: 0.9341 - val_loss: 0.0313 - val_auc: 0.9238\n",
      "Epoch 16: early stopping\n",
      "47/47 [==============================] - 5s 47ms/step\n",
      "Run 3/5, Test AUC: 0.7230\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 36s 37ms/step - loss: 0.0376 - auc: 0.8998 - val_loss: 0.0381 - val_auc: 0.8968\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 25s 36ms/step - loss: 0.0352 - auc: 0.9082 - val_loss: 0.0369 - val_auc: 0.9013\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 24s 35ms/step - loss: 0.0341 - auc: 0.9135 - val_loss: 0.0363 - val_auc: 0.9035\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 25s 35ms/step - loss: 0.0332 - auc: 0.9174 - val_loss: 0.0361 - val_auc: 0.9057\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 25s 35ms/step - loss: 0.0327 - auc: 0.9199 - val_loss: 0.0356 - val_auc: 0.9064\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 25s 35ms/step - loss: 0.0321 - auc: 0.9227 - val_loss: 0.0351 - val_auc: 0.9089\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0316 - auc: 0.9252 - val_loss: 0.0349 - val_auc: 0.9096\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0312 - auc: 0.9272 - val_loss: 0.0348 - val_auc: 0.9105\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0308 - auc: 0.9291 - val_loss: 0.0344 - val_auc: 0.9133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0304 - auc: 0.9309 - val_loss: 0.0344 - val_auc: 0.9133\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0301 - auc: 0.9324 - val_loss: 0.0341 - val_auc: 0.9151\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0298 - auc: 0.9338 - val_loss: 0.0342 - val_auc: 0.9142\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0296 - auc: 0.9348 - val_loss: 0.0340 - val_auc: 0.9149\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0292 - auc: 0.9363 - val_loss: 0.0340 - val_auc: 0.9150\n",
      "Epoch 14: early stopping\n",
      "47/47 [==============================] - 5s 48ms/step\n",
      "Run 4/5, Test AUC: 0.6980\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 36s 37ms/step - loss: 0.0392 - auc: 0.8894 - val_loss: 0.0352 - val_auc: 0.9075\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 22s 32ms/step - loss: 0.0355 - auc: 0.9053 - val_loss: 0.0338 - val_auc: 0.9133\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0343 - auc: 0.9115 - val_loss: 0.0332 - val_auc: 0.9154\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0335 - auc: 0.9152 - val_loss: 0.0327 - val_auc: 0.9185\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0329 - auc: 0.9185 - val_loss: 0.0325 - val_auc: 0.9194\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0323 - auc: 0.9216 - val_loss: 0.0320 - val_auc: 0.9220\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0319 - auc: 0.9236 - val_loss: 0.0317 - val_auc: 0.9231\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0314 - auc: 0.9265 - val_loss: 0.0314 - val_auc: 0.9240\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0311 - auc: 0.9279 - val_loss: 0.0311 - val_auc: 0.9256\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0307 - auc: 0.9295 - val_loss: 0.0313 - val_auc: 0.9255\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0305 - auc: 0.9310 - val_loss: 0.0310 - val_auc: 0.9265\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0301 - auc: 0.9329 - val_loss: 0.0307 - val_auc: 0.9268\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0298 - auc: 0.9341 - val_loss: 0.0309 - val_auc: 0.9264\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0296 - auc: 0.9348 - val_loss: 0.0305 - val_auc: 0.9284\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0293 - auc: 0.9359 - val_loss: 0.0306 - val_auc: 0.9281\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0291 - auc: 0.9374 - val_loss: 0.0306 - val_auc: 0.9289\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0289 - auc: 0.9381 - val_loss: 0.0305 - val_auc: 0.9289\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0287 - auc: 0.9386 - val_loss: 0.0306 - val_auc: 0.9288\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0285 - auc: 0.9400 - val_loss: 0.0300 - val_auc: 0.9304\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 22s 31ms/step - loss: 0.0283 - auc: 0.9405 - val_loss: 0.0302 - val_auc: 0.9294\n",
      "47/47 [==============================] - 5s 49ms/step\n",
      "Run 5/5, Test AUC: 0.6930\n",
      "Standard Deviation of AUC over 5 runs: 0.0111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "#from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "\n",
    "# Function to define and compile the model\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Function for Focal Loss\n",
    "#https://www.programmersought.com/article/60001511310/\n",
    "def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "    epsilon = 1.e-7\n",
    "    def loss_fn2(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    \n",
    "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.math.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "    \n",
    "    return loss_fn2\n",
    "\n",
    "# Number of runs to calculate the standard deviation\n",
    "n_runs = 5\n",
    "auc_scores = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Assuming X and y are your complete dataset excluding the test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "    # Preprocess the test set\n",
    "    X_test = preprocess_input(X_test)\n",
    "   \n",
    "    # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "    # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "    # Preprocess the training and validation sets\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_val = preprocess_input(X_val)\n",
    "\n",
    "    model = build_model(input_shape, num_classes)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,  # Adjust based on your dataset and model's performance\n",
    "        batch_size=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1  # Set to 0 to reduce log messages\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate and print the standard deviation of AUC scores\n",
    "auc_std_dev = np.std(auc_scores)\n",
    "print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 10000\n",
    "\n",
    "img_names = []\n",
    "for dis in disease_img.keys():\n",
    "    num = round(number/91324*len(disease_img[dis]))\n",
    "    for i in range(num):\n",
    "        img_names.append(disease_img[dis][i])\n",
    "        \n",
    "X = []\n",
    "train_image = []\n",
    "y = np.zeros(shape = (len(img_names), len(disease_class.keys())))\n",
    "\n",
    "for i in tqdm(range(len(img_names))):\n",
    "    \n",
    "    img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/Dataset/'+img_names[i],target_size=(224,224,3))\n",
    "    img = image.img_to_array(img)\n",
    "    train_image.append(img)\n",
    "    \n",
    "    for j in range(len(disease_class.keys())):\n",
    "        if disease_rev[j+1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "            y[i][j] = 1\n",
    "            \n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 30001/30001 [01:18<00:00, 382.97it/s]\n",
      "2024-03-11 17:07:23.864827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 17:07:28.332516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:19:00.0, compute capability: 8.6\n",
      "2024-03-11 17:07:28.334326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22288 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2024-03-11 17:07:28.336140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22288 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:67:00.0, compute capability: 8.6\n",
      "2024-03-11 17:07:28.337879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22228 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:68:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 17:08:26.769848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n",
      "2024-03-11 17:08:28.552131: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-11 17:08:30.319915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-11 17:08:30.324863: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5624417064c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-11 17:08:30.324902: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 17:08:30.324908: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 17:08:30.324914: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 17:08:30.324918: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 17:08:30.331965: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-11 17:08:30.469564: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-11 17:08:30.547900: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 69s 28ms/step - loss: 0.0356 - auc: 0.9067 - val_loss: 0.0330 - val_auc: 0.9193\n",
      "Epoch 2/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0321 - auc: 0.9228 - val_loss: 0.0319 - val_auc: 0.9240\n",
      "Epoch 3/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0308 - auc: 0.9286 - val_loss: 0.0318 - val_auc: 0.9236\n",
      "Epoch 4/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0299 - auc: 0.9325 - val_loss: 0.0311 - val_auc: 0.9272\n",
      "Epoch 5/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0293 - auc: 0.9354 - val_loss: 0.0311 - val_auc: 0.9271\n",
      "Epoch 6/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0287 - auc: 0.9379 - val_loss: 0.0314 - val_auc: 0.9297\n",
      "Epoch 7/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0282 - auc: 0.9399 - val_loss: 0.0312 - val_auc: 0.9270\n",
      "Epoch 8/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0278 - auc: 0.9417 - val_loss: 0.0309 - val_auc: 0.9299\n",
      "Epoch 9/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0273 - auc: 0.9438 - val_loss: 0.0310 - val_auc: 0.9281\n",
      "Epoch 10/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0270 - auc: 0.9448 - val_loss: 0.0309 - val_auc: 0.9286\n",
      "Epoch 11/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0267 - auc: 0.9460 - val_loss: 0.0304 - val_auc: 0.9304\n",
      "Epoch 12/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0263 - auc: 0.9476 - val_loss: 0.0304 - val_auc: 0.9298\n",
      "Epoch 13/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0260 - auc: 0.9487 - val_loss: 0.0307 - val_auc: 0.9290\n",
      "Epoch 14/20\n",
      "2100/2100 [==============================] - 47s 22ms/step - loss: 0.0257 - auc: 0.9497 - val_loss: 0.0305 - val_auc: 0.9297\n",
      "Epoch 14: early stopping\n",
      "141/141 [==============================] - 8s 42ms/step\n",
      "Run 1/3, Test AUC: 0.7531\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "\n",
    "for number in range(30000, 31000, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized224/' + img_names[i], target_size=(224, 224, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "\n",
    "    # Function to define and compile the model\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs = 3\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "\n",
    "        model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            batch_size=10,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(X_test)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "    # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    aauc=np.mean(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "    print(\"aauc=\",aauc)\n",
    "\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 91324/91324 [01:26<00:00, 1060.56it/s]\n",
      "2024-03-11 15:19:18.674747: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 15:19:23.036140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:19:00.0, compute capability: 8.6\n",
      "2024-03-11 15:19:23.037948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22288 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2024-03-11 15:19:23.039726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22288 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:67:00.0, compute capability: 8.6\n",
      "2024-03-11 15:19:23.046243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22228 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:68:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 15:20:07.453157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n",
      "2024-03-11 15:20:09.232888: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-11 15:20:10.991902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-11 15:20:10.998759: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fd52802baa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-11 15:20:10.998809: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 15:20:10.998816: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 15:20:10.998821: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 15:20:10.998826: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-11 15:20:11.005915: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-11 15:20:11.145250: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-11 15:20:11.224288: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6393/6393 [==============================] - 127s 18ms/step - loss: 0.0366 - auc: 0.9067 - val_loss: 0.0331 - val_auc: 0.9197\n",
      "Epoch 2/20\n",
      "6393/6393 [==============================] - 107s 17ms/step - loss: 0.0328 - auc: 0.9210 - val_loss: 0.0325 - val_auc: 0.9220\n",
      "Epoch 3/20\n",
      "6393/6393 [==============================] - 107s 17ms/step - loss: 0.0317 - auc: 0.9259 - val_loss: 0.0326 - val_auc: 0.9215\n",
      "Epoch 4/20\n",
      "6393/6393 [==============================] - 107s 17ms/step - loss: 0.0309 - auc: 0.9291 - val_loss: 0.0326 - val_auc: 0.9236\n",
      "Epoch 5/20\n",
      "6393/6393 [==============================] - 108s 17ms/step - loss: 0.0303 - auc: 0.9317 - val_loss: 0.0320 - val_auc: 0.9230\n",
      "Epoch 6/20\n",
      "6393/6393 [==============================] - 109s 17ms/step - loss: 0.0299 - auc: 0.9335 - val_loss: 0.0319 - val_auc: 0.9238\n",
      "Epoch 7/20\n",
      "6393/6393 [==============================] - 108s 17ms/step - loss: 0.0295 - auc: 0.9352 - val_loss: 0.0325 - val_auc: 0.9215\n",
      "Epoch 8/20\n",
      "6393/6393 [==============================] - 108s 17ms/step - loss: 0.0291 - auc: 0.9366 - val_loss: 0.0320 - val_auc: 0.9242\n",
      "Epoch 9/20\n",
      "6393/6393 [==============================] - 108s 17ms/step - loss: 0.0288 - auc: 0.9379 - val_loss: 0.0315 - val_auc: 0.9262\n",
      "Epoch 10/20\n",
      "6393/6393 [==============================] - 107s 17ms/step - loss: 0.0286 - auc: 0.9389 - val_loss: 0.0321 - val_auc: 0.9256\n",
      "Epoch 11/20\n",
      "6393/6393 [==============================] - 107s 17ms/step - loss: 0.0284 - auc: 0.9399 - val_loss: 0.0317 - val_auc: 0.9255\n",
      "Epoch 12/20\n",
      "6393/6393 [==============================] - 107s 17ms/step - loss: 0.0281 - auc: 0.9409 - val_loss: 0.0324 - val_auc: 0.9231\n",
      "Epoch 12: early stopping\n",
      "429/429 [==============================] - 8s 15ms/step\n",
      "Run 1/1, Test AUC: 0.7040\n",
      "Standard Deviation of AUC over 1 runs: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "\n",
    "for number in range(91324, 92324, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized112/' + img_names[i], target_size=(112, 112, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)\n",
    "    \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "    #from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "    from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "    #from tensorflow.keras.applications.densenet import preprocess_input\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    input_shape = (112, 112, 3)  # Example input shape for a typical image dataset\n",
    "    num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "\n",
    "    # Function to define and compile the model\n",
    "    def build_model(input_shape, num_classes):\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "    # Function for Focal Loss\n",
    "    #https://www.programmersought.com/article/60001511310/\n",
    "    def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "        epsilon = 1.e-7\n",
    "        def loss_fn2(y_true, y_pred):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "            y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "            ce = -tf.math.log(y_t)\n",
    "            weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "            fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "            loss = tf.reduce_mean(fl)\n",
    "            return loss\n",
    "\n",
    "        return loss_fn2\n",
    "\n",
    "    # Number of runs to calculate the standard deviation\n",
    "    n_runs = 1\n",
    "    auc_scores = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Assuming X and y are your complete dataset excluding the test set\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "        # Preprocess the test set\n",
    "        X_test = preprocess_input(X_test)\n",
    "\n",
    "        # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "        # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "        # Preprocess the training and validation sets\n",
    "        X_train = preprocess_input(X_train)\n",
    "        X_val = preprocess_input(X_val)\n",
    "\n",
    "        model = build_model(input_shape, num_classes)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max', verbose=1)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=20,  # Adjust based on your dataset and model's performance\n",
    "            batch_size=10,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1  # Set to 0 to reduce log messages\n",
    "        )\n",
    "\n",
    "        # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "        y_pred = model.predict(X_test)\n",
    "        auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "    # Calculate and print the standard deviation of AUC scores\n",
    "    auc_std_dev = np.std(auc_scores)\n",
    "    print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "\n",
    "        # Now, X and y contain the images and labels for this iteration\n",
    "        # You can now proceed with training or saving this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 10000/10000 [01:32<00:00, 108.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming disease_img, disease_class, and simp_data_ref are predefined\n",
    "for number in range(10000, 11000, 10000): \n",
    "    print(number)# From 10,000 to 90,000\n",
    "    img_names = []\n",
    "    for dis in disease_img.keys():\n",
    "        num = round(number / 91324 * len(disease_img[dis]))\n",
    "        for i in range(num):\n",
    "            img_names.append(disease_img[dis][i])\n",
    "            \n",
    "    X = []\n",
    "    train_image = []\n",
    "    y = np.zeros(shape=(len(img_names), len(disease_class.keys())))\n",
    "\n",
    "    for i in tqdm(range(len(img_names))):\n",
    "        img = image.load_img('/media/ntu/volume1/home/s123md305_01/Documents/CombinedResized/Resized224/' + img_names[i], target_size=(224, 224, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        train_image.append(img)\n",
    "        \n",
    "        for j in range(len(disease_class.keys())):\n",
    "            if disease_rev[j + 1] == simp_data_ref['Finding Labels'][img_names[i]]:\n",
    "                y[i][j] = 1\n",
    "                \n",
    "    X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "700/700 [==============================] - 31s 42ms/step - loss: 0.0912 - auc: 0.8048 - val_loss: 0.0654 - val_auc: 0.8562\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0645 - auc: 0.8551 - val_loss: 0.0583 - val_auc: 0.8646\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0573 - auc: 0.8656 - val_loss: 0.0539 - val_auc: 0.8744\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0524 - auc: 0.8757 - val_loss: 0.0501 - val_auc: 0.8810\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0487 - auc: 0.8827 - val_loss: 0.0484 - val_auc: 0.8865\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0459 - auc: 0.8898 - val_loss: 0.0461 - val_auc: 0.8907\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0437 - auc: 0.8953 - val_loss: 0.0448 - val_auc: 0.8934\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0418 - auc: 0.9005 - val_loss: 0.0443 - val_auc: 0.8924\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0402 - auc: 0.9051 - val_loss: 0.0425 - val_auc: 0.8971\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0387 - auc: 0.9092 - val_loss: 0.0420 - val_auc: 0.9002\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0375 - auc: 0.9129 - val_loss: 0.0414 - val_auc: 0.9009\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0365 - auc: 0.9159 - val_loss: 0.0406 - val_auc: 0.9058\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0355 - auc: 0.9189 - val_loss: 0.0404 - val_auc: 0.9055\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0347 - auc: 0.9213 - val_loss: 0.0393 - val_auc: 0.9052\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0338 - auc: 0.9244 - val_loss: 0.0390 - val_auc: 0.9072\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0331 - auc: 0.9264 - val_loss: 0.0388 - val_auc: 0.9056\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0324 - auc: 0.9286 - val_loss: 0.0387 - val_auc: 0.9053\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0318 - auc: 0.9298 - val_loss: 0.0376 - val_auc: 0.9109\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0312 - auc: 0.9322 - val_loss: 0.0377 - val_auc: 0.9097\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0307 - auc: 0.9335 - val_loss: 0.0374 - val_auc: 0.9117\n",
      "47/47 [==============================] - 10s 145ms/step\n",
      "Run 1/3, Test AUC: 0.6606\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 31s 42ms/step - loss: 0.1005 - auc: 0.7825 - val_loss: 0.0699 - val_auc: 0.8462\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0644 - auc: 0.8548 - val_loss: 0.0608 - val_auc: 0.8599\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0569 - auc: 0.8673 - val_loss: 0.0563 - val_auc: 0.8670\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0521 - auc: 0.8765 - val_loss: 0.0529 - val_auc: 0.8757\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0486 - auc: 0.8835 - val_loss: 0.0497 - val_auc: 0.8792\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0458 - auc: 0.8901 - val_loss: 0.0477 - val_auc: 0.8831\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0435 - auc: 0.8958 - val_loss: 0.0463 - val_auc: 0.8851\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0416 - auc: 0.9008 - val_loss: 0.0455 - val_auc: 0.8861\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0400 - auc: 0.9052 - val_loss: 0.0438 - val_auc: 0.8955\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0387 - auc: 0.9089 - val_loss: 0.0427 - val_auc: 0.8948\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0374 - auc: 0.9126 - val_loss: 0.0419 - val_auc: 0.8959\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0363 - auc: 0.9155 - val_loss: 0.0411 - val_auc: 0.8991\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0354 - auc: 0.9183 - val_loss: 0.0405 - val_auc: 0.9000\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0345 - auc: 0.9212 - val_loss: 0.0400 - val_auc: 0.9023\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0336 - auc: 0.9240 - val_loss: 0.0406 - val_auc: 0.8979\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0330 - auc: 0.9261 - val_loss: 0.0396 - val_auc: 0.9003\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0322 - auc: 0.9283 - val_loss: 0.0387 - val_auc: 0.9069\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0317 - auc: 0.9303 - val_loss: 0.0386 - val_auc: 0.9082\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0312 - auc: 0.9317 - val_loss: 0.0382 - val_auc: 0.9064\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0305 - auc: 0.9340 - val_loss: 0.0379 - val_auc: 0.9069\n",
      "47/47 [==============================] - 4s 79ms/step\n",
      "Run 2/3, Test AUC: 0.6650\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 31s 42ms/step - loss: 0.0791 - auc: 0.8351 - val_loss: 0.0653 - val_auc: 0.8590\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0626 - auc: 0.8574 - val_loss: 0.0575 - val_auc: 0.8688\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0559 - auc: 0.8685 - val_loss: 0.0528 - val_auc: 0.8778\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0518 - auc: 0.8763 - val_loss: 0.0498 - val_auc: 0.8822\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 27s 38ms/step - loss: 0.0484 - auc: 0.8832 - val_loss: 0.0476 - val_auc: 0.8820\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0460 - auc: 0.8892 - val_loss: 0.0457 - val_auc: 0.8902\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0438 - auc: 0.8941 - val_loss: 0.0444 - val_auc: 0.8917\n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0422 - auc: 0.8986 - val_loss: 0.0434 - val_auc: 0.8935\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0406 - auc: 0.9029 - val_loss: 0.0424 - val_auc: 0.8971\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0392 - auc: 0.9065 - val_loss: 0.0419 - val_auc: 0.8985\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0381 - auc: 0.9101 - val_loss: 0.0404 - val_auc: 0.9002\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0369 - auc: 0.9133 - val_loss: 0.0401 - val_auc: 0.9007\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0360 - auc: 0.9165 - val_loss: 0.0395 - val_auc: 0.9041\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0351 - auc: 0.9192 - val_loss: 0.0388 - val_auc: 0.9063\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0343 - auc: 0.9217 - val_loss: 0.0383 - val_auc: 0.9085\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0336 - auc: 0.9237 - val_loss: 0.0377 - val_auc: 0.9089\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0329 - auc: 0.9260 - val_loss: 0.0377 - val_auc: 0.9085\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0323 - auc: 0.9278 - val_loss: 0.0370 - val_auc: 0.9100\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0317 - auc: 0.9299 - val_loss: 0.0368 - val_auc: 0.9106\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 27s 39ms/step - loss: 0.0312 - auc: 0.9314 - val_loss: 0.0371 - val_auc: 0.9085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 4s 79ms/step\n",
      "Run 3/3, Test AUC: 0.6528\n",
      "Standard Deviation of AUC over 3 runs: 0.0050\n",
      "aauc= 0.6594708127982392\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "#from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "input_shape = (224, 224, 3)  # Example input shape for a typical image dataset\n",
    "num_classes = 15  # Change this to match the number of classes in your dataset\n",
    "\n",
    "    # Function to define and compile the model\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform', dtype='float32')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Function for Focal Loss\n",
    "#https://www.programmersought.com/article/60001511310/\n",
    "def focal_loss(alpha = 0.5, beta = 2.0):\n",
    "    epsilon = 1.e-7\n",
    "    def loss_fn2(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.math.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), beta)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "\n",
    "    return loss_fn2\n",
    "\n",
    "# Number of runs to calculate the standard deviation\n",
    "n_runs = 3\n",
    "auc_scores = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Assuming X and y are your complete dataset excluding the test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42+run)\n",
    "\n",
    "    # Preprocess the test set\n",
    "    X_test = preprocess_input(X_test)\n",
    "\n",
    "    # Split the training + validation set into actual training and validation sets (82.35:17.65)\n",
    "    # This will give you 70% of the total data for training and 15% of the total data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42+run)\n",
    "\n",
    "    # Preprocess the training and validation sets\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_val = preprocess_input(X_val)\n",
    "\n",
    "    model = build_model(input_shape, num_classes)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=3, verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,  # Adjust based on your dataset and model's performance\n",
    "        batch_size=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1  # Set to 0 to reduce log messages\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on your test set, assuming X_test, y_test are your test data and labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred, average = 'macro')\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Run {run+1}/{n_runs}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate and print the standard deviation of AUC scores\n",
    "auc_std_dev = np.std(auc_scores)\n",
    "aauc=np.mean(auc_scores)\n",
    "print(f\"Standard Deviation of AUC over {n_runs} runs: {auc_std_dev:.4f}\")\n",
    "print(\"aauc=\",aauc)\n",
    "\n",
    "    # Now, X and y contain the images and labels for this iteration\n",
    "    # You can now proceed with training or saving this data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
